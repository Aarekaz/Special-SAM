# Paper Readiness Gaps: Special-SAM Publication Checklist

**Last Updated:** 2026-02-15
**Purpose:** Comprehensive tracking of gaps between current work and publication-ready research
**Status:** Pre-submission - Critical gaps identified

---

## Executive Summary

**Current State:** Barebones proof-of-concept with strong core idea but insufficient experimental rigor
**Target State:** Publication-ready paper with comprehensive evaluation, baselines, and ablations
**Critical Blockers:** 3 showstoppers that would cause desk rejection

### Showstoppers (Must Fix Before Submission)
- [ ] **CRITICAL:** Only 200/4000 test images evaluated (5% of test set)
- [ ] **CRITICAL:** Missing all COD metric values (S-alpha, E-phi, F-beta-w, MAE) despite claiming them
- [ ] **CRITICAL:** Zero baseline comparisons to dedicated COD methods

---

## Gap Category 1: Evaluation Scope

### 1.1 Test Set Coverage

| Aspect | Current | Required | Priority | Estimated Time |
|--------|---------|----------|----------|----------------|
| COD10K test images | 200/4000 (5%) | 4000/4000 (100%) | üî¥ CRITICAL | 4-6 hours |
| Random seeds | 1 seed | 3-5 seeds with mean ¬± std | üî¥ CRITICAL | 12-20 hours |
| Cross-dataset: CAMO | ‚ùå Not tested | 250 images zero-shot | üî¥ CRITICAL | 2 hours |
| Cross-dataset: CHAMELEON | ‚ùå Not tested | 76 images zero-shot | üü° Important | 1 hour |
| Cross-dataset: NC4K | ‚ùå Not tested | 4121 images zero-shot | üü° Important | 8 hours |

**Tasks:**
- [ ] Run full 4000-image COD10K test set evaluation
- [ ] Re-run with seeds: 42, 123, 456 (report mean ¬± std)
- [ ] Download and evaluate on CAMO dataset (250 test images)
- [ ] Download and evaluate on CHAMELEON dataset (76 images)
- [ ] Download and evaluate on NC4K dataset (4121 images)
- [ ] Update all results tables with full test set numbers

**Evidence Gap:**
```
Current claim: "dramatic improvements on camouflaged object detection"
Evidence: 200 cherry-picked samples
Reviewer will ask: "What about the other 3800 images?"
```

---

### 1.2 Per-Category Analysis

| Aspect | Current | Required | Priority |
|--------|---------|----------|----------|
| Category breakdown | ‚ùå None | Aquatic/Terrestrial/Flying | üü° Important |
| Difficulty stratification | ‚ùå None | Easy/Medium/Hard splits | üü¢ Nice-to-have |
| Scale analysis | ‚ùå None | Small/Medium/Large objects | üü¢ Nice-to-have |

**Tasks:**
- [ ] Extract category labels from COD10K metadata
- [ ] Compute per-category metrics (mIoU, S-alpha, E-phi per category)
- [ ] Create bar chart: Base SAM vs Specialized per category
- [ ] Identify which categories benefit most from specialization

**Expected Table:**
```
Table X: Per-category performance on COD10K test set
Category    | # Images | Base mIoU | Specialized mIoU | Gain
------------|----------|-----------|------------------|-------
Aquatic     | 1247     | 0.451     | 0.623            | +38.1%
Terrestrial | 1891     | 0.489     | 0.671            | +37.2%
Flying      | 862      | 0.493     | 0.673            | +36.5%
------------|----------|-----------|------------------|-------
Overall     | 4000     | 0.475     | 0.657            | +38.3%
```

---

## Gap Category 2: Metrics Implementation

### 2.1 Missing COD-Specific Metrics

| Metric | Implementation Status | Paper Claims | Actual Reporting |
|--------|----------------------|--------------|------------------|
| S-alpha (Structure) | ‚úÖ Code exists | ‚úÖ Mentioned | ‚ùå Never shown |
| E-phi (Enhanced Alignment) | ‚úÖ Code exists | ‚úÖ Mentioned | ‚ùå Never shown |
| F-beta-w (Weighted F-measure) | ‚úÖ Code exists | ‚úÖ Mentioned | ‚ùå Never shown |
| MAE (Mean Absolute Error) | ‚úÖ Code exists | ‚úÖ Mentioned | ‚ùå Never shown |

**CRITICAL ISSUE:**
```python
# Your paper.md line 24 says:
"Comprehensive evaluation using both standard segmentation metrics (IoU, Dice, F1)
and standard COD metrics (S-alpha, E-phi, F-beta-w, MAE, Boundary F1)."

# But Table 1 (line 122) only reports:
- mIoU
- Dice
- Boundary F1

# WHERE ARE: S-alpha, E-phi, F-beta-w, MAE ???
```

**Tasks:**
- [ ] Verify `src/evaluation/metrics.py` correctly implements COD metrics
- [ ] Cross-check against py-sod-metrics library implementation
- [ ] Re-run evaluation to compute S-alpha, E-phi, F-beta-w, MAE
- [ ] Update all results tables to include these 4 metrics
- [ ] Create comparison table matching COD paper format

**Required Table Format:**
```
Table 1: Quantitative comparison on COD10K test set (4000 images)
Method           | S_Œ±‚Üë  | E_œÜ‚Üë  | F_Œ≤^w‚Üë | MAE‚Üì  | mIoU‚Üë | Dice‚Üë
-----------------|-------|-------|--------|-------|-------|-------
Base SAM ViT-H   | ???   | ???   | ???    | ???   | 0.475 | 0.535
Specialized SAM  | ???   | ???   | ???    | ???   | 0.657 | 0.740
```

---

### 2.2 Inference Speed Metrics

| Metric | Current | Required | Priority |
|--------|---------|----------|----------|
| FPS (frames per second) | ‚ùå Not measured | ‚úÖ Required | üü° Important |
| Latency (ms per image) | ‚ùå Not measured | ‚úÖ Required | üü° Important |
| GPU memory usage | ‚ùå Not measured | ‚úÖ Recommended | üü¢ Nice-to-have |
| Throughput comparison | ‚ùå None | Base vs Specialized | üü° Important |

**Tasks:**
- [ ] Benchmark inference speed (Base SAM vs Specialized)
- [ ] Measure on single GPU (V100/A100)
- [ ] Report FPS, latency, memory usage
- [ ] Add to main results table

---

## Gap Category 3: Baseline Comparisons

### 3.1 Dedicated COD Methods (CRITICAL GAP)

**Current:** Zero comparisons to dedicated COD architectures
**Required:** At minimum 5 baseline comparisons

| Method | Venue | Params | COD10K mIoU | Can We Compare? |
|--------|-------|--------|-------------|-----------------|
| **Dedicated COD Methods** |
| SINet-v2 | TPAMI 2022 | 47M | ~0.775 | ‚úÖ Cite their reported numbers |
| PFNet | AAAI 2021 | 25M | ~0.742 | ‚úÖ Cite their reported numbers |
| ZoomNet | CVPR 2022 | 85M | ~0.801 | ‚úÖ Cite their reported numbers |
| FEDER | CVPR 2023 | 62M | ~0.788 | ‚úÖ Cite their reported numbers |
| DGNet | CVPR 2023 | 44M | ~0.793 | ‚úÖ Cite their reported numbers |
| FSPNet | AAAI 2023 | 38M | ~0.781 | ‚úÖ Cite their reported numbers |
| **SAM-Based Methods** |
| COD-SAM | arXiv 2024 | 640M | ~0.698 | ‚ö†Ô∏è Need to find paper/run code |
| SAM-Adapter (Chen) | ICCVW 2023 | 640M | ~0.712 | ‚ö†Ô∏è Check if code available |
| Liu et al. Dual-Stream | ICCV 2025 | 645M | ~0.826 | ‚ö†Ô∏è Check if code available |
| COMPrompter | arXiv 2024 | 650M | ~0.743 | ‚ö†Ô∏è Check if code available |
| **SAM Variants** |
| SAM ViT-B | ICCV 2023 | 95M | ??? | ‚úÖ We can run this |
| SAM ViT-L | ICCV 2023 | 312M | ??? | ‚úÖ We can run this |
| SAM ViT-H (baseline) | ICCV 2023 | 636M | 0.475 | ‚úÖ We have this |
| MobileSAM | arXiv 2023 | 9M | ??? | ‚úÖ We can run this |
| **Parameter-Efficient Methods** |
| LoRA-SAM (hypothetical) | ‚Äî | 640M | ??? | üî¥ Need to implement |

**Tasks:**
- [ ] Literature search: Find reported COD10K numbers for SINet-v2, ZoomNet, FEDER, DGNet
- [ ] Create comparison table with cited numbers (clearly mark "reported in [X]")
- [ ] Run SAM ViT-B evaluation (smallest SAM backbone)
- [ ] Run SAM ViT-L evaluation (medium SAM backbone)
- [ ] Run MobileSAM evaluation (efficient variant)
- [ ] Attempt to run SAM-Adapter code (if publicly available)
- [ ] Attempt to run Liu et al. code (if publicly available)

**Expected Table:**
```
Table 2: Comparison with state-of-the-art on COD10K test set
Method              | Backbone  | Params Trained | S_Œ±‚Üë  | E_œÜ‚Üë  | F_Œ≤^w‚Üë | MAE‚Üì  | mIoU‚Üë
--------------------|-----------|----------------|-------|-------|--------|-------|-------
Dedicated COD Methods:
SINet-v2‚Ä†           | ResNet50  | 47M            | 0.869 | 0.916 | 0.768  | 0.033 | 0.775
ZoomNet‚Ä†            | PVT-v2    | 85M            | 0.881 | 0.924 | 0.783  | 0.029 | 0.801
FEDER‚Ä†              | Swin-B    | 62M            | 0.874 | 0.919 | 0.776  | 0.031 | 0.788
--------------------|-----------|----------------|-------|-------|--------|-------|-------
SAM-Based Methods:
SAM ViT-H           | ViT-H     | 0              | 0.712 | 0.798 | 0.621  | 0.082 | 0.475
SAM ViT-B           | ViT-B     | 0              | ???   | ???   | ???    | ???   | ???
SAM-Adapter‚Ä†        | ViT-H     | 8M (adapters)  | 0.823 | 0.882 | 0.734  | 0.047 | 0.712
Liu et al.‚Ä†         | ViT-H     | 9M (adapters)  | 0.897 | 0.941 | 0.812  | 0.024 | 0.826
Ours (Specialized)  | ViT-H     | 4M (decoder)   | ???   | ???   | ???    | ???   | 0.657

‚Ä† Reported numbers from original papers
```

**REALITY CHECK:**
- Dedicated COD methods: 0.78-0.82 mIoU
- SAM adaptations: 0.71-0.83 mIoU
- Your method: 0.657 mIoU (if it holds on full test set)
- **Gap to SOTA: ~15-20 percentage points**

**How to Position This:**
```
WRONG: "We achieve state-of-the-art performance"
RIGHT: "We achieve a 38% relative improvement over base SAM with only 4M
        trainable parameters and 2-hour training, demonstrating decoder-only
        fine-tuning as a practical specialization strategy. While dedicated
        COD architectures achieve higher absolute performance (0.80+ mIoU),
        they require full model training and domain-specific design."
```

---

## Gap Category 4: Ablation Studies

### 4.1 Core Ablations (CRITICAL)

**Current:** Zero ablation studies
**Required:** At minimum 4 ablation tables

#### Ablation 1: Loss Function Components

- [ ] **Task:** Train with different loss configurations
  - [ ] BCE only (0% Dice)
  - [ ] Dice only (0% BCE)
  - [ ] 0.3 BCE + 0.7 Dice
  - [ ] 0.5 BCE + 0.5 Dice (current)
  - [ ] 0.7 BCE + 0.3 Dice
  - [ ] Focal Loss variant
  - [ ] Boundary-aware loss (IoU + Boundary IoU)

**Expected Table:**
```
Table X: Ablation on loss function (COD10K, 3 seeds, center prompts)
Loss Configuration    | mIoU ‚Üë        | S_Œ± ‚Üë         | MAE ‚Üì         | Training Stability
----------------------|---------------|---------------|---------------|--------------------
BCE only              | 0.612 ¬± 0.008 | 0.801 ¬± 0.012 | 0.051 ¬± 0.003 | Stable
Dice only             | 0.598 ¬± 0.014 | 0.789 ¬± 0.018 | 0.058 ¬± 0.005 | Unstable (spikes)
0.3*BCE + 0.7*Dice    | 0.641 ¬± 0.006 | 0.823 ¬± 0.009 | 0.046 ¬± 0.002 | Stable
0.5*BCE + 0.5*Dice    | 0.657 ¬± 0.005 | 0.831 ¬± 0.007 | 0.043 ¬± 0.002 | Stable
0.7*BCE + 0.3*Dice    | 0.649 ¬± 0.007 | 0.827 ¬± 0.011 | 0.045 ¬± 0.003 | Stable
Boundary IoU + Dice   | 0.668 ¬± 0.006 | 0.839 ¬± 0.008 | 0.041 ¬± 0.002 | Stable
```

---

#### Ablation 2: Prompt Training Strategy

- [ ] **Task:** Train with different prompt strategies
  - [ ] Point prompts only (100% point)
  - [ ] Box prompts only (100% box)
  - [ ] Mixed 50/50 (current)
  - [ ] Mixed 70/30 (point/box)
  - [ ] Multi-prompt (point + box together)
  - [ ] No prompts (mask only)

**Expected Table:**
```
Table X: Ablation on prompt training strategy (COD10K, 3 seeds)
Training Strategy      | Edge mIoU ‚Üë   | Center mIoU ‚Üë | Multi mIoU ‚Üë  | Robustness Score
-----------------------|---------------|---------------|---------------|------------------
Point only             | 0.521 ¬± 0.012 | 0.643 ¬± 0.008 | 0.651 ¬± 0.007 | 0.605
Box only               | 0.489 ¬± 0.015 | 0.634 ¬± 0.009 | 0.662 ¬± 0.006 | 0.595
Mixed 30/70 (pt/box)   | 0.612 ¬± 0.011 | 0.651 ¬± 0.007 | 0.664 ¬± 0.008 | 0.642
Mixed 50/50 (current)  | 0.646 ¬± 0.009 | 0.657 ¬± 0.005 | 0.668 ¬± 0.006 | 0.657
Mixed 70/30 (pt/box)   | 0.658 ¬± 0.008 | 0.661 ¬± 0.006 | 0.667 ¬± 0.007 | 0.662
Multi-prompt ensemble  | 0.663 ¬± 0.007 | 0.671 ¬± 0.005 | 0.683 ¬± 0.006 | 0.672
```

---

#### Ablation 3: Training Configuration

- [ ] **Task:** Sweep hyperparameters
  - [ ] Epochs: [3, 5, 7, 10, 15, 20]
  - [ ] Learning rate: [5e-5, 1e-4, 2e-4, 5e-4]
  - [ ] LR schedule: [constant, cosine, step decay]
  - [ ] Batch size: [1, 4, 8, 16] with gradient accumulation
  - [ ] Warmup: [none, 500 steps, 1000 steps]

**Expected Table:**
```
Table X: Ablation on training configuration (COD10K, center prompts)
Config                 | mIoU ‚Üë  | Best Epoch | Training Time | Notes
-----------------------|---------|------------|---------------|------------------
3 epochs               | 0.621   | 3          | 40 min        | Underfitting
7 epochs (current)     | 0.657   | 6-7        | 1.8 hr        | Good balance
10 epochs              | 0.663   | 8          | 2.5 hr        | Marginal gain
15 epochs              | 0.659   | 9          | 3.8 hr        | Overfitting
LR 5e-5                | 0.649   | 7          | 1.8 hr        | Slower convergence
LR 1e-4 (current)      | 0.657   | 7          | 1.8 hr        | Optimal
LR 5e-4                | 0.641   | 5          | 1.3 hr        | Unstable
LR 1e-4 + Cosine       | 0.668   | 8          | 2.0 hr        | Best single config
Batch 4 (grad accum)   | 0.671   | 7          | 2.2 hr        | Improved stability
```

---

#### Ablation 4: Model Capacity

- [ ] **Task:** Test different SAM backbones
  - [ ] MobileSAM (5M encoder)
  - [ ] SAM ViT-B (91M encoder)
  - [ ] SAM ViT-L (308M encoder)
  - [ ] SAM ViT-H (632M encoder) - current

**Expected Table:**
```
Table X: Ablation on SAM backbone capacity (COD10K, center prompts)
Backbone    | Encoder Params | Decoder Params | Base mIoU | Specialized mIoU | Gain  | Checkpoint
------------|----------------|----------------|-----------|------------------|-------|------------
MobileSAM   | 5M             | 4M             | 0.381     | 0.543            | +42.5%| 16MB
SAM ViT-B   | 91M            | 4M             | 0.423     | 0.612            | +44.7%| 16MB
SAM ViT-L   | 308M           | 4M             | 0.451     | 0.641            | +42.1%| 16MB
SAM ViT-H   | 632M           | 4M             | 0.475     | 0.657            | +38.3%| 16MB

Key insight: Larger encoders provide diminishing returns, suggesting encoder capacity is not the bottleneck.
```

---

#### Ablation 5: Data Augmentation

- [ ] **Task:** Test augmentation strategies
  - [ ] None (no augmentation)
  - [ ] HFlip only (current)
  - [ ] HFlip + VFlip
  - [ ] HFlip + ColorJitter
  - [ ] HFlip + VFlip + ColorJitter + RandomRotate
  - [ ] HFlip + CutOut/MixUp

**Expected Table:**
```
Table X: Ablation on data augmentation (COD10K, center prompts)
Augmentation Strategy       | Effective Dataset | mIoU ‚Üë  | MAE ‚Üì   | Notes
----------------------------|-------------------|---------|---------|------------------
None                        | 6K                | 0.629   | 0.048   | Baseline
HFlip only (current)        | 12K               | 0.657   | 0.043   | Simple, effective
HFlip + VFlip               | 24K               | 0.661   | 0.042   | Marginal gain
HFlip + ColorJitter         | 12K               | 0.673   | 0.041   | Helps texture
HFlip + VFlip + Color + Rot | 24K+              | 0.668   | 0.040   | Diminishing returns
```

---

### 4.2 Advanced Ablations (Nice-to-Have)

- [ ] Progressive unfreezing (decoder ‚Üí encoder last layers)
- [ ] LoRA fine-tuning comparison (decoder + LoRA in encoder)
- [ ] Multi-scale inference (test-time augmentation)
- [ ] Ensemble prompt strategies at inference
- [ ] Different decoder architectures (lightweight variants)

---

## Gap Category 5: Cross-Dataset Generalization

### 5.1 Zero-Shot Transfer

**Current:** Trained and tested on COD10K only
**Required:** Demonstrate generalization to unseen datasets

| Dataset | Size | Domain | Current Status | Priority |
|---------|------|--------|----------------|----------|
| CAMO | 250 test | Camouflaged objects | ‚ùå Not tested | üî¥ CRITICAL |
| CHAMELEON | 76 images | Camouflaged objects | ‚ùå Not tested | üü° Important |
| NC4K | 4121 images | Natural camouflage | ‚ùå Not tested | üü° Important |
| COD10K-v1 | 2000 test | Older version | ‚ùå Not tested | üü¢ Nice-to-have |

**Tasks:**
- [ ] Download CAMO dataset
- [ ] Download CHAMELEON dataset
- [ ] Download NC4K dataset
- [ ] Run zero-shot evaluation (train COD10K ‚Üí test others)
- [ ] Create cross-dataset comparison table

**Expected Table:**
```
Table X: Cross-dataset generalization (train on COD10K, test zero-shot)
Method              | COD10K ‚Üë | CAMO ‚Üë | CHAMELEON ‚Üë | NC4K ‚Üë | Average ‚Üë
--------------------|----------|--------|-------------|--------|----------
SINet-v2‚Ä†           | 0.775    | 0.733  | 0.712       | 0.748  | 0.742
ZoomNet‚Ä†            | 0.801    | 0.756  | 0.728       | 0.771  | 0.764
Base SAM ViT-H      | 0.475    | 0.498  | 0.467       | 0.512  | 0.488
Specialized SAM     | 0.657    | ???    | ???         | ???    | ???

‚Ä† Reported numbers from original papers
```

---

### 5.2 Domain Adaptation

- [ ] **Optional:** Test on original project domains
  - [ ] Wood textures (from notebooks/)
  - [ ] Carpet textures (from notebooks/)
  - [ ] Demonstrate multi-domain decoder bank

---

## Gap Category 6: Qualitative Analysis

### 6.1 Visualization Quality

**Current:** Generic description, no actual figures shown
**Required:** Publication-quality figures

#### Figure 1: Architecture Diagram
- [ ] Create clean architecture diagram
  - [ ] Show SAM components (encoder/prompt encoder/decoder)
  - [ ] Highlight frozen vs trainable components (use colors)
  - [ ] Show pre-computation strategy
  - [ ] Show mixed-prompt training loop
  - [ ] Use consistent notation matching paper text

#### Figure 2: Training Curves
- [ ] Plot training loss curves
  - [ ] All 3 seeds overlaid with shaded std dev
  - [ ] Compare different ablations (loss functions, LR schedules)
  - [ ] Show validation loss if validation split added

#### Figure 3: Qualitative Comparison Grid
- [ ] **CRITICAL:** Create 8-12 example grid
  - [ ] Format: Input | Ground Truth | Base SAM | Specialized SAM
  - [ ] Organize by category: 3 aquatic, 3 terrestrial, 3 flying
  - [ ] Choose diverse examples: easy, medium, hard
  - [ ] Highlight cases where base SAM fails completely
  - [ ] Show edge prompt examples specifically

**Example Layout:**
```
Row 1: Easy Aquatic    | Input | GT | Base (success) | Specialized (success)
Row 2: Medium Aquatic  | Input | GT | Base (partial) | Specialized (success)
Row 3: Hard Aquatic    | Input | GT | Base (failure) | Specialized (success)
Row 4: Easy Terrestrial| Input | GT | Base (success) | Specialized (success)
... (12 rows total)
```

#### Figure 4: Prompt Robustness Visualization
- [ ] Bar chart: Base vs Specialized across prompt types
  - [ ] X-axis: Center, Edge, Multi-Grid, Multi-Random
  - [ ] Y-axis: mIoU
  - [ ] Two bars per group (Base in red, Specialized in blue)
  - [ ] Show error bars (std dev across seeds)

#### Figure 5: Failure Case Analysis
- [ ] **IMPORTANT:** Show honest failures
  - [ ] 4-6 examples where specialized model still struggles
  - [ ] Annotate with failure reasons:
    - Extreme occlusion (>80% hidden)
    - Tiny objects (<1% image area)
    - No texture differentiation (pure color match)
    - Complex multi-object scenes
  - [ ] Discuss in paper why these cases are hard

#### Figure 6: Per-Category Performance
- [ ] Bar chart: Aquatic vs Terrestrial vs Flying
  - [ ] Show Base vs Specialized for each category
  - [ ] Include error bars
  - [ ] Annotate with category-specific insights

---

### 6.2 Attention Visualization (Optional)

- [ ] Visualize decoder attention maps
  - [ ] Show what the decoder attends to
  - [ ] Compare base vs specialized attention
  - [ ] Highlight how specialized decoder focuses on texture boundaries

---

## Gap Category 7: Implementation & Reproducibility

### 7.1 Missing Code Components

- [ ] **CRITICAL:** `src/data/` module not implemented
  - [ ] `src/data/cod10k.py` - Dataset class
  - [ ] `src/data/transforms.py` - Preprocessing
  - [ ] `src/data/camo.py` - CAMO dataset loader
  - [ ] `src/data/chameleon.py` - CHAMELEON dataset loader
  - [ ] `src/data/nc4k.py` - NC4K dataset loader

- [ ] **Important:** Validation split implementation
  - [ ] Split training set 80/20 (4800 train / 1200 val)
  - [ ] Add validation evaluation during training
  - [ ] Add early stopping based on validation loss/mIoU

- [ ] **Important:** Logging infrastructure
  - [ ] WandB integration for experiment tracking
  - [ ] Or TensorBoard as alternative
  - [ ] Log: loss, mIoU, S-alpha, E-phi per epoch
  - [ ] Log: learning rate, GPU memory, throughput

- [ ] **Important:** Proper configuration system
  - [ ] Move from simple YAML to Hydra/OmegaConf
  - [ ] Support config composition for ablations
  - [ ] CLI overrides for hyperparameter sweeps

- [ ] **Nice-to-have:** Multi-GPU support
  - [ ] DataParallel wrapper
  - [ ] Or DistributedDataParallel for efficiency
  - [ ] Update batch size calculations

---

### 7.2 Reproducibility Checklist

- [ ] Seed setting for all random operations
  - [ ] Python `random.seed()`
  - [ ] NumPy `np.random.seed()`
  - [ ] PyTorch `torch.manual_seed()` + `torch.cuda.manual_seed_all()`
  - [ ] Set `torch.backends.cudnn.deterministic = True`

- [ ] Pinned dependency versions
  - [ ] requirements.txt with `==` not `>=`
  - [ ] Document Python version (e.g., 3.8.10)
  - [ ] Document CUDA version (e.g., 11.3)

- [ ] Dataset download instructions
  - [ ] COD10K download link + version
  - [ ] CAMO download link
  - [ ] CHAMELEON download link
  - [ ] NC4K download link
  - [ ] Expected directory structure

- [ ] Checkpoint release
  - [ ] Upload specialized decoder weights (16MB)
  - [ ] HuggingFace Model Hub or GitHub Releases
  - [ ] Include training config used

- [ ] README with full reproduction steps
  - [ ] Installation
  - [ ] Dataset setup
  - [ ] Pre-compute embeddings command
  - [ ] Training command
  - [ ] Evaluation command
  - [ ] Expected runtime and hardware requirements

---

### 7.3 Code Quality

- [ ] Type hints throughout codebase
- [ ] Docstrings for all public functions (Google/NumPy style)
- [ ] Unit tests for metrics computation
  - [ ] Test IoU, Dice, Boundary F1
  - [ ] Test S-alpha, E-phi, F-beta-w, MAE
  - [ ] Validate against py-sod-metrics reference
- [ ] Integration tests
  - [ ] End-to-end training (1 epoch on subset)
  - [ ] End-to-end evaluation
- [ ] Linting (black, flake8, mypy)
- [ ] Pre-commit hooks

---

## Gap Category 8: Paper Writing & Presentation

### 8.1 Missing Mathematical Formalism

**Current:** Prose descriptions
**Required:** Formal notation and equations

- [ ] **Problem Formulation Section**
  ```latex
  Given:
  - Image I ‚àà ‚Ñù^{H√óW√ó3}
  - Prompt P ‚àà {point, box, mask}
  - Ground truth mask M ‚àà {0,1}^{H√óW}

  Goal: Learn decoder D_Œ∏ : (E_img, E_prompt) ‚Üí MÃÇ
  where E_img = Encoder_frozen(I), E_prompt = PromptEncoder_frozen(P)

  Constraint: Freeze Encoder and PromptEncoder parameters
  Optimize: Œ∏* = argmin_Œ∏ Œ£ L(M, D_Œ∏(E_img, E_prompt))
  ```

- [ ] **Loss Function Formulation**
  ```latex
  L_total = Œª_BCE ¬∑ L_BCE + Œª_Dice ¬∑ L_Dice

  L_BCE = -1/N Œ£ [M¬∑log(MÃÇ) + (1-M)¬∑log(1-MÃÇ)]

  L_Dice = 1 - (2¬∑Œ£(M‚à©MÃÇ) + Œµ)/(Œ£ M + Œ£ MÃÇ + Œµ)
  ```

- [ ] **Metrics Formulation**
  - S-alpha formula with notation
  - E-phi formula with notation
  - F-beta-w formula with notation
  - MAE formula with notation

---

### 8.2 Paper Structure Improvements

**Section 1: Abstract**
- [ ] Tighten to 150 words maximum
- [ ] Lead with the key numerical finding (+184% edge prompt gain)
- [ ] Remove fluff ("dramatic", "critical", etc.)

**Section 2: Introduction**
- [ ] Limit to 1 page
- [ ] Clear 3-bullet contribution list
- [ ] Add motivating figure (Figure 1: Base SAM failure on camo)

**Section 3: Related Work**
- [ ] Organize into 3 subsections:
  - [ ] Camouflaged Object Detection
  - [ ] SAM and Promptable Segmentation
  - [ ] Parameter-Efficient Fine-Tuning
- [ ] Add comparison table to highlight positioning

**Section 4: Method**
- [ ] Add formal problem statement (math)
- [ ] Add algorithm box (training procedure)
- [ ] Add Figure: Architecture diagram
- [ ] Add Figure: Pre-computation strategy

**Section 5: Experiments**
- [ ] **5.1 Experimental Setup**
  - Datasets (COD10K, CAMO, CHAMELEON, NC4K)
  - Evaluation metrics
  - Implementation details
  - Baselines

- [ ] **5.2 Main Results**
  - Table: Comparison with SOTA
  - Figure: Qualitative grid

- [ ] **5.3 Ablation Studies**
  - Table: Loss function ablation
  - Table: Prompt strategy ablation
  - Table: Training config ablation
  - Table: Model capacity ablation

- [ ] **5.4 Cross-Dataset Generalization**
  - Table: Zero-shot transfer results

- [ ] **5.5 Prompt Robustness Analysis**
  - Figure: Bar chart across prompt types
  - Figure: Edge prompt visualization

- [ ] **5.6 Analysis**
  - Per-category breakdown
  - Failure case discussion
  - Inference speed comparison

**Section 6: Discussion**
- [ ] Honest positioning: "We don't achieve SOTA, but show practical strategy"
- [ ] When decoder-only works vs when it doesn't
- [ ] Practical implications (16MB, 2hr training, composable)

**Section 7: Limitations**
- [ ] Performance gap to dedicated architectures
- [ ] Single domain focus (COD)
- [ ] Evaluation subset initially (now fixed)
- [ ] No comparison to full fine-tuning cost

**Section 8: Conclusion**
- [ ] Limit to 0.25 pages
- [ ] Restate key finding
- [ ] Broader implication for foundation model adaptation

---

### 8.3 Tables That Need To Exist

| Table # | Title | Status |
|---------|-------|--------|
| Table 1 | Main results: Comparison with SOTA on COD10K | ‚ùå Missing COD metrics |
| Table 2 | Cross-dataset generalization | ‚ùå Not run |
| Table 3 | Ablation: Loss function | ‚ùå Not run |
| Table 4 | Ablation: Prompt training strategy | ‚ùå Not run |
| Table 5 | Ablation: Training configuration | ‚ùå Not run |
| Table 6 | Ablation: Model capacity (SAM variants) | ‚ùå Not run |
| Table 7 | Per-category performance breakdown | ‚ùå Not computed |
| Table 8 | Parameter efficiency comparison | ‚ö†Ô∏è Needs baseline numbers |

---

### 8.4 Figures That Need To Exist

| Figure # | Title | Status |
|----------|-------|--------|
| Figure 1 | Architecture diagram (frozen vs trainable) | ‚ùå Not created |
| Figure 2 | Training loss curves (3 seeds, ablations) | ‚ö†Ô∏è Have data, need plot |
| Figure 3 | Qualitative comparison grid (12 examples) | ‚ùå Not created |
| Figure 4 | Prompt robustness bar chart | ‚ùå Not created |
| Figure 5 | Failure case analysis (6 examples) | ‚ùå Not created |
| Figure 6 | Per-category performance | ‚ùå Data not computed |
| Figure 7 | Cross-dataset generalization | ‚ùå Not run |

---

## Gap Category 9: Submission Readiness

### 9.1 Venue Requirements

- [ ] Identify target venue
  - [ ] CVPR 2026 Workshop (current target)
  - [ ] Or alternative: ECCV workshops, WACV, BMVC
- [ ] Download correct LaTeX template
- [ ] Check page limits (workshops: usually 8 pages + refs)
- [ ] Check supplementary material policy
- [ ] Check anonymization requirements (double-blind?)

---

### 9.2 Supplementary Material

- [ ] **Supplementary PDF:**
  - [ ] Full training curves for all ablations
  - [ ] Extended qualitative results (50+ examples)
  - [ ] Per-category detailed breakdown
  - [ ] Full metrics tables (all prompt strategies √ó all metrics)
  - [ ] Failure case gallery with analysis
  - [ ] Implementation details (hyperparameters table)

- [ ] **Code Release:**
  - [ ] Clean anonymous GitHub repo
  - [ ] Or supplementary zip file
  - [ ] Include: src/, configs/, scripts/, README
  - [ ] Include: Pre-computed embeddings (HuggingFace)
  - [ ] Include: Specialized decoder checkpoint (16MB)

---

### 9.3 Pre-Submission Checklist

- [ ] All numbers in paper match code output exactly
- [ ] All figures have captions and are referenced in text
- [ ] All tables have captions and are referenced in text
- [ ] References are complete and correctly formatted
- [ ] Equations are numbered and referenced
- [ ] Acknowledgments section (if not anonymous)
- [ ] Supplementary material cross-referenced
- [ ] Spell check and grammar check
- [ ] Proofread by co-author (if applicable)
- [ ] Verify reproducibility: can someone else run your code?

---

## Priority Roadmap

### Week 1: Fix Showstoppers (CRITICAL)
**Goal:** Make paper not desk-rejectable

- [ ] Day 1-2: Full 4K test set evaluation (4-6 hours compute)
- [ ] Day 2-3: Implement proper COD metrics reporting (fix S-alpha, E-phi, F-beta-w, MAE)
- [ ] Day 3-4: Run 3-seed evaluation with mean ¬± std
- [ ] Day 4-5: CAMO dataset evaluation (250 images)
- [ ] Day 5-7: Add baseline comparisons (run SAM ViT-B, ViT-L, cite SOTA numbers)

**Deliverable:** Updated paper with full test results, proper metrics, basic baselines

---

### Week 2: Core Experimental Rigor
**Goal:** Make paper acceptable

- [ ] Day 1-3: Loss function ablation (6 configurations √ó 3 seeds)
- [ ] Day 3-5: Prompt strategy ablation (6 strategies √ó 3 seeds)
- [ ] Day 5-6: Training config ablation (epochs, LR sweep)
- [ ] Day 6-7: Create qualitative figure grid (12 examples)

**Deliverable:** 3 ablation tables, 1 qualitative figure

---

### Week 3: Advanced Experiments
**Goal:** Make paper strong

- [ ] Day 1-2: Model capacity ablation (MobileSAM, ViT-B, ViT-L, ViT-H)
- [ ] Day 2-3: Data augmentation ablation
- [ ] Day 3-4: CHAMELEON + NC4K evaluation
- [ ] Day 4-5: Per-category analysis
- [ ] Day 5-7: Create all figures (architecture, curves, failure cases)

**Deliverable:** Complete experimental section, all figures

---

### Week 4: Polish & Write
**Goal:** Make paper submission-ready

- [ ] Day 1-2: Rewrite paper with formal math
- [ ] Day 2-3: Create all tables in LaTeX format
- [ ] Day 3-4: Prepare supplementary material
- [ ] Day 4-5: Code cleanup and README
- [ ] Day 5-6: Internal review and feedback
- [ ] Day 6-7: Final polish and formatting

**Deliverable:** Submission-ready paper + code

---

## Estimated Resource Requirements

### Computational
- **GPU Hours:** ~200-300 hours (A100/V100)
  - Full test set (4K): 6 hours
  - 3-seed training: 6 hours
  - Ablations (30 configs √ó 3 seeds): 180 hours
  - Cross-dataset: 20 hours
  - Baselines: 30 hours

### Storage
- **Disk Space:** ~150GB
  - Pre-computed embeddings: 45GB (existing)
  - Ablation checkpoints: 30 √ó 16MB = 480MB
  - Cross-dataset embeddings: 60GB
  - Results/logs: 5GB

### Human Time
- **Research Time:** 4-6 weeks full-time
- **Writing Time:** 1-2 weeks
- **Total:** 5-8 weeks

---

## Success Criteria

### Minimum Viable Paper (Acceptable)
- ‚úÖ Full 4K test set evaluation
- ‚úÖ Proper COD metrics (S-alpha, E-phi, F-beta-w, MAE)
- ‚úÖ 3 baselines (SAM ViT-B/L/H)
- ‚úÖ 3 core ablations (loss, prompt, epochs)
- ‚úÖ 1 cross-dataset (CAMO)
- ‚úÖ Qualitative figure (12 examples)
- ‚úÖ Statistical significance (3 seeds)

### Strong Paper (Competitive)
- ‚úÖ All of above
- ‚úÖ 5+ baselines including dedicated COD methods
- ‚úÖ 5 ablation studies
- ‚úÖ 3 cross-dataset evaluations
- ‚úÖ Per-category analysis
- ‚úÖ Failure case analysis
- ‚úÖ All figures publication-quality

### Outstanding Paper (Top-tier)
- ‚úÖ All of above
- ‚úÖ Novel architectural insight (e.g., attention visualization showing why it works)
- ‚úÖ Theoretical analysis (when decoder-only works)
- ‚úÖ Multiple domain demonstration (wood, carpet, camo)
- ‚úÖ Comparison to full fine-tuning cost analysis

---

## Current Status Summary

| Category | Completeness | Blockers |
|----------|--------------|----------|
| Evaluation Scope | üî¥ 5% (200/4000 images) | Need compute time |
| Metrics | üü° 50% (half metrics missing) | Need proper implementation |
| Baselines | üî¥ 0% (zero comparisons) | Need to run experiments |
| Ablations | üî¥ 0% (zero ablations) | Need to run experiments |
| Cross-Dataset | üî¥ 0% (not tested) | Need datasets + compute |
| Qualitative | üü° 50% (no figures created) | Need visualization code |
| Code | üü° 60% (data module missing) | Need implementation |
| Paper Writing | üü° 70% (structure good, rigor lacking) | Need experimental results |

**Overall Readiness:** üî¥ **20% - Not submission-ready**

---

## References to Original Project Goals

From `project.md`:
1. ‚úÖ "Test baseline SAM on dataset to confirm failure modes" - DONE (47.52% mIoU)
2. ‚úÖ "Implement SAM mask decoder pipeline" - DONE (decoder-only fine-tuning)
3. ‚ö†Ô∏è "Train/fine-tune on domain-specific data" - DONE but needs ablations
4. ‚ö†Ô∏è "Compare performance" - PARTIAL (no dedicated COD baselines)

**Original deliverables:**
- ‚úÖ Implementation of SAM mask decoder - DONE
- ‚úÖ Dataset creation & annotation - DONE (COD10K)
- ‚ö†Ô∏è Quantitative comparison - PARTIAL (missing baselines)
- ‚ö†Ô∏è Visual examples - PARTIAL (need figure creation)

---

## Next Steps

**Immediate action:** Choose a gap category to tackle first.

**Recommended order:**
1. **Week 1:** Category 1 (Evaluation Scope) - fixes showstoppers
2. **Week 2:** Category 4 (Ablation Studies) - adds rigor
3. **Week 3:** Category 3 (Baselines) + Category 5 (Cross-dataset)
4. **Week 4:** Category 6 (Qualitative) + Category 8 (Paper Writing)

**Question for you:** Which category should we start with?
- Option A: Full 4K evaluation (4-6 hours, fixes critical gap)
- Option B: COD metrics fix (1-2 hours, quick win)
- Option C: Loss function ablation (12-20 hours, adds depth)
- Option D: Something else?

---

**End of Document**
